{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4300cc67-7cfa-4552-a73b-0a6b7f7dbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ec2a84-b0a9-48e5-8591-040407942db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load Named Entity Recognition pipeline with DistilBERT\n",
    "ner_pipeline = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22749627-f273-4f04-8d7a-d9e3bbed3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = '''\n",
    "\"In 2017, researchers at Google AI introduced the Transformer architecture, \n",
    "which became the foundation for models like BERT and GPT. DARPA has since funded\n",
    "multiple projects on Explainable AI (XAI) to improve interpretability in deep \n",
    "learning models.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c54e538-0d83-4f36-a0aa-d877ece66ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: ĠGoogle, Label: ORG, Score: 0.9981, Start: 26, End: 32\n",
      "Entity: ĠAI, Label: ORG, Score: 0.9975, Start: 33, End: 35\n",
      "Entity: ĠTrans, Label: MISC, Score: 0.9918, Start: 51, End: 56\n",
      "Entity: former, Label: MISC, Score: 0.9897, Start: 56, End: 62\n",
      "Entity: ĠB, Label: MISC, Score: 0.9954, Start: 122, End: 123\n",
      "Entity: ERT, Label: MISC, Score: 0.9941, Start: 123, End: 126\n",
      "Entity: ĠG, Label: MISC, Score: 0.9958, Start: 131, End: 132\n",
      "Entity: PT, Label: MISC, Score: 0.9943, Start: 132, End: 134\n",
      "Entity: ĠDAR, Label: ORG, Score: 0.9993, Start: 136, End: 139\n",
      "Entity: PA, Label: ORG, Score: 0.9994, Start: 139, End: 141\n",
      "Entity: ĠExplain, Label: MISC, Score: 0.8437, Start: 180, End: 187\n",
      "Entity: able, Label: MISC, Score: 0.6033, Start: 187, End: 191\n",
      "Entity: ĠAI, Label: MISC, Score: 0.8188, Start: 192, End: 194\n",
      "Entity: X, Label: MISC, Score: 0.9306, Start: 196, End: 197\n",
      "Entity: AI, Label: MISC, Score: 0.9738, Start: 197, End: 199\n"
     ]
    }
   ],
   "source": [
    "# Run NER\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Print detected entities\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.4f}, Start: {entity['start']}, End: {entity['end']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
